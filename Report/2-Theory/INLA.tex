\newpage
\section{INLA: Integrated Nested Laplace Approximations}
The Integrated Nested Laplace Approximation (\inla from this point) is a method for approximate Bayesian inference on latent Gaussian models (LGM), first proposed by \textcite{rue2009inla}. \inla is made readily available through the \texttt{R} package \texttt{R-INLA} and a thorough explanation of the method and the related framework can be found at \url{https://www.r-inla.org}. 
Bayesian inference with \inla is applicable to the class of Bayesian models named Latent Gaussian Models (LGMs) \parencite{rue2009inla}.  

\newpar LGMs are hierarchical models, where the observations $\Vector{y}$ are conditionally independent given a latent gaussian random field $\Vector{x}$ and some hyper parameters $\boldsymbol{\theta}$.
LGMs are usually expressed through a predictor $\eta_i$ related to the mean $\mu_i$ of observation $y_i$ through some known link function $g^{-1}(\cdot)$:
\begin{equation}
    \mu_i = g^{-1}(\eta_i).
    \label{eq:LGMlinkFunction}
\end{equation}
The general form of the predictor is
\begin{equation}
    \eta_\text{i} = \alpha + \sum_{\text{j}=1}^{n_{\beta}}\beta_j z_{\text{ji}} + \sum_{k=1}^{n_f}f^{(k)}(u_{\text{ki}}) + \epsilon_{i}
    \label{LGM-linear-predictor}
\end{equation}
\parencite{rue2009inla}. Here, $\alpha$ is the intercept, $\boldsymbol{\beta}$ are the linear effects on some covariates \textbf{z}, the $f^{(k)}$´s are random effects on some covariates \textbf{u} and $\epsilon_i$ is the error term. The model can be summarized as 
\begin{equation}
    \begin{aligned}
    \bm{y}\mid \bm{x}, \boldsymbol{\theta} &\sim \prod_{\text{i}} \pi (y_{\text{i}} \mid \eta_{\text{i}}, \boldsymbol{\theta})\\
    \bm{x} \mid \boldsymbol{\theta} &\sim \Normal(\Vector{0}, \bm{Q}^{-1}(\boldsymbol{\theta})) \\
    \boldsymbol{\theta} &\sim \pi(\boldsymbol{\theta}).
    \end{aligned}
    \label{LGM-model-summarized}
\end{equation}
In the \inla-framework, the latent field $\mb{x}$ is given by
\begin{equation}
    \bm{x} = (\boldsymbol{\eta}, \alpha, \boldsymbol{\beta}, \boldsymbol{f})
    \label{LGM-latent-field}
\end{equation}
\parencite{martinoRiebler2019}. The element that distinguishes LGMs from other Bayesian models is the assumed Gaussian prior on the latent field $\Vector{x}$ \parencite{rue2009inla}. Here $\Matrix{Q}(\Vector{\theta})$ is the precision matrix. $\Vector{\theta}$ may have any suitable prior distribution. 

\newpar The \inla method uses the properties of the LGM and models the latent effects $\mathbf{x}$ as a Gaussian Markov Random Field (GMRF). The methodology gives approximations to the marginal posterior distributions of the latent field $\mathbf{x}$ and the associated hyperparameters $\boldsymbol{\theta}$. The main advantage of \inla compared to other methods for Bayesian inference is its computational power. This is a result of the GMRF assumption on the latent field, which induces sparsity in $\Matrix{Q}(\Vector{\theta})$, and enables fast computations as long as $\Vector{\theta}$ is not too large \parencite{martinoRiebler2019}. Compared to the widely used Markov Chain Monte Carlo (MCMC) method for instance, it has been shown that \inla is able to produce more accurate results in seconds or minutes than MCMC is able to produce in hours or days \parencite{rue2009inla}.
This often makes \inla, when applicable, the preferred method of inference when working with model fitting, as it allows for efficient comparison of model fits and it opens up the possibility of sensitivity analysis, comparing the results when varying the prior distributions. Still, the usefulness of \inla is restricted by the requirements on the models it is applicable to, meaning the scope of the \inla method exclude some widely used models. \textcolor{myDarkGreen}{Keep the meaning, but update the formulation of the last sentence. }

\subsection{Restrictions of the \inla framework}
\label{sec:InlaRestrictions}
A limitation of the \inla approach is that it imposes some restrictions on the model used in the Bayesian inference. These restrictions can be formulated in three requirements:
\begin{enumerate}
    \item It should be possible to write the model as an LGM
    \item Each observation $y_i$ should depend on the latent field $\textbf{x}$ only through the predictor $\eta_i$ 
    \item The predictors $\eta_i$ of the LGM should depend linearly on $\mu$, $\Vector{\beta}$ and $\Vector{f}$
    \label{item:inlaRequirements}
\end{enumerate}
\parencite{martinoRiebler2019}.
\newpar The APC-model fulfills all requirements listed above \parencite{rieblerHeld2010}, and thus it is possible to use \inla for Bayesian inference on APC-models. This has also already been widely and successfully done (see e.g. \textcite{RieblerThesis2010}). The Lee-Carter models, on the other hand, does not fulfill these requirements. The multiplicative term in the expression fro the predictor, $\beta_x\cdot \kappa_t$ (from Expression \ref{eq:LC-model}) conflicts with the requirement that the predictior $\eta_i$ should be linear.  
   
%A limitation of the \inla method is that it is only applicable to a specific set of models, namely LGMs as described in Section \ref{sec:LGM}. Even though a substantial number of models can be expressed as an LGM (\see \cite{rue2009inla} for a description of problems easily solved by \inla), this does leave out some widely used models. One model where \inla is not applicable is the Lee-Carter model for mortality prediction, described in Section \ref{section:Lee-Carter}.

%\subsection{Gaussian Markov Random Field}
%[Include this section in some sentences in the INLA theory]
%In its approximation procedure, the INLA methodology (often) assumes a Gaussian Markov Random Field structure on the latent field $\bm{x}$. This is differs from a regular Gaussian Random Field in that the markov property of the GMRF ensures some spatial conditional independence in the latent effect $\bm{x}$. More specifically, it means that $x_i$ and $x_j$ will be conditionally independent given the remaining latent effect $\bm{x}_{-ij}$ for many $i,j$ \cite{rue2016bayesian}. This leads to a sparse precision matrix $\bm{Q}(\boldsymbol{\theta})$ and INLA utilizes this property to speed up the calculation. [DETTE ER SKREVET VELDIG FRA EGET HODE, MEST FOR Å VITE SÅNN ISJ HVA SOM SKAL STÅ.]  
