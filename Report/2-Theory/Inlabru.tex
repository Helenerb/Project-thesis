\newpage
\section{Linearization of the predictor in the Lee-Carter model}
\label{sec:inlabru}
For models that fulfill all requirements needed to use the \inla method except for the requirement of a linear predictor $\eta_{x,t}$, it seems possible to find a way around this obstacle. \textcolor{myDarkGreen}{this sentence needs to be changed, but I canÂ´t think of a better formulation. } \textcite{BachlLindgren2019} propose a solution to this kind of problem with a method named \inlabru, originally developed for ecological surveys containing spatially correlated data. They find a linearization if the non-linear predictor through a fixed-point iteration using \inla, and then setting this linearization as the predictor in the final \inla run. Their approach is implemented in the \texttt{R} package \textttt{inlabru}, and information about installation and usage of this package can be found at \cite{Inlabru}. Here, we outline the approach of \textcite{BachlLindgren2019} for the linearization and following model approximations of LGMs with non-linear predictors. For a more thorough explanation of the method, and the application to ecological data, we refer to their original paper \parencite{BachlLindgren2019} and the article \textcite{Inlabru}. 

\newpar Assuming that we have an LGM, with structure as described in \ref{LGM-model-summarized}, but where the likelihood depends on a non-linear predictor $\tilde{\boldsymbol{\eta}}$:
\begin{equation}
    \textbf{y}\mid\textbf{x},\boldsymbol{\theta} \sim p(\textbf{y}\mid \tilde{\boldsymbol{\eta}}(\Vector{x}), \boldsymbol{\theta}),
    \label{Eq:non-linear-predictor}
\end{equation}
where $\Vector{x}$ is the latent field. 
\inlabru then uses a linearization of $\tilde{\boldsymbol{\eta}}$ to be able to successfully run \inla. The linearization is done using a Taylor approximation around some point $\textbf{x}_0$:
\begin{equation}
    \bar{\boldsymbol{\eta}} = \boldsymbol{\eta}_{x,t}(\textbf{x}_0) + B(\textbf{x} - \textbf{x}_0),
\end{equation}
where $B$ is the derivative matrix of $\boldsymbol{\eta}$ evaluated at $\textbf{x}_0$. This linearized predictor is then used to run \inla, by substituting $\bar{\Vector{\eta}}(\Vector{x})$ for $\tilde{\Vector{\eta}}(\Vector{x})$ in the likelihood models to obtain an approximation for the likelihood:
\begin{equation}
    \bar{p}(\Vector{y}\mid \Vector{x}, \Vector{\theta}) = p(\Vector{y}\mid \bar{\Vector{\eta}}, \Vector{\theta}) \approx p(\Vector{y} \mid \tilde{\Vector{\eta}}).
\end{equation}
By running \inla with this approximated likelihood, approximated posterior distributions are obtained for the latent effects, hyperparameters and the predictor $\tilde{\Vector{\eta}}$.
\inlabru finds the optimal linearization point $\textbf{x}_0$ through a fixed-point iteration with \inla. For each step $s$ in the iteration, the point $\Vector{x}'$ is found that that maximizes the posterior distribution for the latent effects that resulted from the \inla approximation using the linearization from the last step $\textbf{x}_{s-1}$. We denote this posterior approximated distribution as $\bar{p}_{\Vector{x}_{s-1}}(\Vector{x}\mid \Vector{y}, \Vector{\theta})$. The fixed-point update for the next linearization point $\Vector{x}_s$ is then 
\begin{equation}
    \begin{aligned}
    \Vector{x}' = \text{argmax}_{\Vector{x}}\,\,\bar{p}_{\Vector{x}_{s-1}}(\Vector{x}\mid \Vector{y}, \hat{\Vector{\theta}}) =: f(\bar{p}_{\Vector{x}_{s-1}}), \quad \hat{\Vector{\theta}} = \text{argmax}_{\Vector{\theta}}\,\,\bar{p}_{\Vector{x}_{s-1}}(\Vector{\theta}\mid \Vector{y}).\\
    \Vector{x}_s = (1 - \lambda)\Vector{x}_{s-1} + \lambda\Vector{x}',\quad \text{where $\lambda$ minimize}\quad \norm{\tilde{\Vector{\eta}}(\Vector{x}_s) - \bar{\Vector{\eta}}(\Vector{x}')}.
    \end{aligned}
\end{equation}
Inlabru runs these iterations until it reaches approximate convergence:
\begin{equation}
\Vector{x}_s \approx f(\bar{p}_{\Vector{x}_{s-1}})
\end{equation}\parencite{Inlabru}.

